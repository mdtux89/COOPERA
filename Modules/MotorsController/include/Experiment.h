/*
 * Copyright (C) 2013 Marco Damonte
 * CopyPolicy: Released under the terms of the LGPLv2.1 or later, see LGPL.TXT
 * Author: Marco Damonte - mdtux89@gmail.com (University of Genoa, Italy), 2013
 * Based on work of Shashank Pathak for Reinforcement Learning algorithm
 */

#ifndef EXPERIMENT_H
#define EXPERIMENT_H

#include <Configuration.h>
#include <StatesActions.h>
#include <Sarsa.h>

/**
 * Sarsa RL experiment: it generates a policy for the robot to follow.
 * The RL task is defined through a Json configuration file and the initial state is
 * passed in the class constructor.
 */
class Experiment : public yarp::os::Thread
{
protected:
    yarp::sig::Vector resetState, targetState;
    string qTableName;
    SarsaAlgo *learner;
    vector<vector<int> > plan;
    int m_testEpisodes;
    int m_testRuns;


public:

    Experiment(vector<int> initial);

    ~Experiment();

    virtual void run();

    /**
     * Get the learner thread
     * @return the learner thread
     */
    SarsaAlgo* getLearner()
    {
        return (learner);
    }

    /**
     * Get the plan generated by the experiment
     * @return the plan
     */
    vector<vector<int> > getPlan(){
        return plan;
    }

};

#endif // EXPERIMENT_H
